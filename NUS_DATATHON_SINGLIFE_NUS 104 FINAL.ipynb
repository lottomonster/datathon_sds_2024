{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas \n",
    "#%pip install matplotlib\n",
    "#%pip install scikit-learn\n",
    "#%pip install imblearn\n",
    "#%pip install tensorflow\n",
    "#%pip install keras-tuner -q\n",
    "#%pip install imbalanced-learn\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import math\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner\n",
    "\n",
    "\n",
    "## Neural Network model\n",
    "\n",
    "def existing_model(units, dropout, lr):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(units = units, activation = \"relu\"))\n",
    "    if dropout:\n",
    "        keras.layers.Dropout(0.25)\n",
    "    model.add(keras.layers.Dense(12, activation = \"relu\"))\n",
    "    if dropout:\n",
    "        keras.layers.Dropout(0.25)\n",
    "    model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate = lr), loss = 'binary_crossentropy', \n",
    "                  metrics = ['accuracy', 'F1Score', 'R2Score', keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "## Hyper parameters to be investigated\n",
    "\n",
    "def build_model(hp):\n",
    "    units = hp.Int(\"units\", min_value=24, max_value=60, step=4)\n",
    "    dropout = hp.Boolean(\"dropout\")\n",
    "    lr = hp.Float(\"lr\", min_value=1e-5, max_value=1e-1, sampling=\"log\") \n",
    "    model = existing_model(units=units, dropout=dropout, lr=lr)\n",
    "    return model\n",
    "\n",
    "    tuner = keras_tuner.BayesianOptimization(\n",
    "        hypermodel=build_model,\n",
    "        objective=keras_tuner.Objective(\"val_f1_score\", direction = \"max\"),\n",
    "        max_trials=3,\n",
    "        executions_per_trial=3,\n",
    "        overwrite=True,\n",
    "        directory=\"tmp\",\n",
    "        project_name=\"helloworld\"\n",
    "    )\n",
    "\n",
    "\n",
    "    build_model(keras_tuner.HyperParameters())\n",
    "\n",
    "    tuner.search(x_train_new, y_train_new, epochs = 25, validation_data = (x, y), batch_size = 5000)\n",
    "\n",
    "    tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import math\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def neural_network(x_train, y_train, x_test):\n",
    "    units = 48\n",
    "    lr = 0.001550047696969543835\n",
    "    dropout = True\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(units = units, activation = \"relu\"))\n",
    "    if dropout:\n",
    "        keras.layers.Dropout(0.25)\n",
    "    model.add(keras.layers.Dense(16, activation = \"relu\"))\n",
    "    if dropout:\n",
    "        keras.layers.Dropout(0.25)\n",
    "    model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate = lr), loss = 'binary_crossentropy', \n",
    "                      metrics = ['accuracy', 'F1Score', 'R2Score', keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    my_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor = 'val_f1_score', patience = 1, mode = 'max'), \n",
    "        # the learning rate will reduce once the F1Score has plateau-ed on a maximum value after 1 epoch\n",
    "        keras.callbacks.EarlyStopping(monitor = 'loss', mode = 'min', patience = 5)\n",
    "        # stops the training once loss is plateau-ed at the minimum\n",
    "    ] \n",
    "    \n",
    "\n",
    "    model.fit(x_train, y_train, epochs = 25, batch_size = 5000)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i][0] < 0.5:\n",
    "            y_pred[i][0] = 0\n",
    "        else:\n",
    "            y_pred[i][0] = 1\n",
    "    \n",
    "    \n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "    rmse = math.sqrt(metrics.mean_squared_error(y_pred,y_test))\n",
    "    print(rmse)\n",
    "\n",
    "    r2score = metrics.r2_score(y_pred, y_test)\n",
    "    print(r2score)\n",
    "\n",
    "    f1score = metrics.f1_score(y_pred, y_test)\n",
    "    print(f1score)\n",
    "\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    \n",
    "    df = hidden_data\n",
    "    \n",
    "    # Change the min_occ_date\n",
    "    \n",
    "    df[\"min_occ_date\"] = pd.to_datetime(df[\"min_occ_date\"],format = \"%Y-%m-%d\",errors='coerce')\n",
    "    df[\"cltdob_fix\"] = pd.to_datetime(df[\"cltdob_fix\"],format = \"%Y-%m-%d\",errors='coerce')\n",
    "\n",
    "    today = datetime.now()\n",
    "\n",
    "    df[\"days_since_first_purchase\"] = df[\"min_occ_date\"].transform(lambda x: today-x)\n",
    "    df[\"age\"] = df[\"cltdob_fix\"].transform(lambda x: today-x)\n",
    "\n",
    "    df[\"days_since_first_purchase\"] = pd.to_numeric(df['days_since_first_purchase'].dt.days, downcast='integer')\n",
    "    df[\"age\"] = pd.to_numeric((df['age'].dt.days)/365, downcast='integer')\n",
    "\n",
    "    df = df.drop(columns=[\"min_occ_date\",\"cltdob_fix\",\"clntnum\"])\n",
    "    \n",
    "    # Selection of data\n",
    "    \n",
    "    cols = df.columns\n",
    "    num_cols = df._get_numeric_data().columns\n",
    "\n",
    "    df[\"f_purchase_lh\"] = df[\"f_purchase_lh\"].fillna(0)\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    cols = df.columns\n",
    "    num_cols = df.select_dtypes(include=numerics)\n",
    "    cat_cols = df.select_dtypes(exclude=numerics)\n",
    "    \n",
    "    # Imputing and Encoding of data\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=2)\n",
    "    num_cols = pd.DataFrame(imputer.fit_transform(num_cols), columns=num_cols.columns)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    for col in cat_cols.columns:\n",
    "        encoded = le.fit_transform(cat_cols[col])    \n",
    "        cat_cols[col] = encoded\n",
    "\n",
    "    df = pd.concat([num_cols.reset_index(drop=True), cat_cols.reset_index(drop=True)], axis=1)\n",
    "    df = df.dropna(thresh=303)\n",
    "    \n",
    "    # Defining of input and output dataframes\n",
    "    \n",
    "    x = df.drop(columns=[\"f_purchase_lh\"])\n",
    "\n",
    "    y = df[\"f_purchase_lh\"]\n",
    "    \n",
    "    # Training 80%, test 20%\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    ## Perform downsampling first on majority\n",
    "    x_train = pd.DataFrame(x_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "\n",
    "    one_indices = y_train[y_train[\"f_purchase_lh\"]==1].index\n",
    "    one = len(y_train[y_train[\"f_purchase_lh\"]==1])\n",
    "\n",
    "    zero_indices = y_train[y_train[\"f_purchase_lh\"]==0].index\n",
    "    zero = len(y_train[y_train[\"f_purchase_lh\"]==0])\n",
    "\n",
    "    random_indices = np.random.choice(zero_indices, zero - 3000 , replace=False)\n",
    "\n",
    "    down_sample_indices = np.concatenate([one_indices,random_indices])\n",
    "\n",
    "    x_train_new = x_train.loc[down_sample_indices]\n",
    "    y_train_new = y_train.loc[down_sample_indices][\"f_purchase_lh\"]\n",
    "\n",
    "\n",
    "    ## Now do SMOTE on minority\n",
    "\n",
    "    x_train_new, y_train_new = SMOTE().fit_resample(x_train_new, y_train_new)\n",
    "\n",
    "\n",
    "    ## Normalise data\n",
    "    scaler = StandardScaler().fit(x_train_new)\n",
    "    x_train_new = scaler.transform(x_train_new)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    x = scaler.transform(x)\n",
    "\n",
    "    result = neural_network(x_train_new, y_train_new, x_test)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "5/5 [==============================] - 2s 17ms/step - loss: 0.6791 - accuracy: 0.6144 - f1_score: 0.6667 - r2_score: 0.0469 - root_mean_squared_error: 0.4881\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor: shape=(), dtype=float32, numpy=0.0>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(filepath)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# test_df = test_df.drop(columns=[\"f_purchase_lh\"])\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(testing_hidden_data(test_df))\n",
      "Cell \u001b[1;32mIn[14], line 100\u001b[0m, in \u001b[0;36mtesting_hidden_data\u001b[1;34m(hidden_data)\u001b[0m\n\u001b[0;32m     97\u001b[0m x_test \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(x_test)\n\u001b[0;32m     98\u001b[0m x \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(x)\n\u001b[1;32m--> 100\u001b[0m result \u001b[38;5;241m=\u001b[39m neural_network(x_train_new, y_train_new, x_test)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[1;32mIn[13], line 39\u001b[0m, in \u001b[0;36mneural_network\u001b[1;34m(x_train, y_train, x_test)\u001b[0m\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate \u001b[38;5;241m=\u001b[39m lr), loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     29\u001b[0m                   metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1Score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2Score\u001b[39m\u001b[38;5;124m'\u001b[39m, keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mRootMeanSquaredError()])\n\u001b[0;32m     31\u001b[0m my_callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     32\u001b[0m     keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(monitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_f1_score\u001b[39m\u001b[38;5;124m'\u001b[39m, patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# the learning rate will reduce once the F1Score has plateau-ed on a maximum value after 1 epoch\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# stops the training once loss is plateau-ed at the minimum\u001b[39;00m\n\u001b[0;32m     36\u001b[0m ] \n\u001b[1;32m---> 39\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m)\n\u001b[0;32m     41\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_pred)):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\ops.py:965\u001b[0m, in \u001b[0;36mTensor.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28mself\u001b[39m, dtype: Optional[dtypes\u001b[38;5;241m.\u001b[39mDType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    963\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    964\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mis_compatible_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    966\u001b[0m         _add_error_prefix(\n\u001b[0;32m    967\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor conversion requested dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    968\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor Tensor with dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    969\u001b[0m             name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    970\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
     ]
    }
   ],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "# test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
